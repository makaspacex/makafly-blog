---
title: 正则化
lang: zh-CN
date: 2024-05-14 10:50:05
author: makaspacex
cover: https://cdn.jsdelivr.net/gh/makaspacex/PictureZone@main/picgo/RegularizationTwoLossFunctions-20240514104631946.svg
tags:
---
# 正则化

## 简单性

###  L2 正则化

请考虑以下**泛化曲线**，该曲线显示的是训练集和验证集相对于训练迭代次数的损失。

![训练集的损失函数逐渐递减。相比之下，验证集的损失函数先下降，然后开始上升。](https://cdn.jsdelivr.net/gh/makaspacex/PictureZone@main/picgo/RegularizationTwoLossFunctions-20240514104631946.svg)

**图 1. 训练集和验证集的损失。**

图 1 显示了一个模型，其中训练损失逐渐减少，但验证损失最终增加。换言之，该泛化曲线显示模型与训练集中的数据[过拟合](https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting?hl=zh-cn)。根据[奥卡姆的内在结构](https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting?hl=zh-cn#ockham)，或许我们可以通过降低复杂模型的复杂度来防止过拟合，这种原则称为**正则化**。

换言之，不应只是以最小化损失（经验风险最小化）为目标：

$$\text{minimize(Loss(Data|Model))}$$

而是以最小化损失和复杂度为目标，这称为**结构风险最小化**：

$$\text{minimize(Loss(Data|Model) + complexity(Model))}$$

现在，我们的训练优化算法是一个由两项内容组成的函数：一个是**损失项**，用于衡量模型与数据的拟合度，另一个是**正则化项**，用于衡量模型复杂度。

机器学习速成课程重点介绍了两种衡量模型复杂度的常见方式（两者在一定程度上相关）：

+   将模型复杂度作为模型中所有特征的权重的函数。
+   将模型复杂度作为具有非零权重的特征总数的函数。（[后面的一个单元](https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization?hl=zh-cn)介绍了此方法。）

如果模型复杂度是权重的函数，则特征权重的绝对值越高，对模型复杂度的贡献就越大。

我们可以使用 ***L2* 正则化**公式量化复杂性，该公式将正则化项定义为所有特征权重的平方和：

$$L_2\text{ regularization term} = ||\boldsymbol w||_2^2 = {w_1^2 + w_2^2 + ... + w_n^2}$$

在此公式中，接近于 0 的权重对模型复杂度几乎没有影响，而离群值权重则会产生巨大的影响。

例如，某个线性模型具有以下权重：

$$\{w_1 = 0.2, w_2 = 0.5, w_3 = 5, w_4 = 1, w_5 = 0.25, w_6 = 0.75\}$$

*L2* 正则化项为 26.915：

$$
\begin{aligned}
w_1^2 + w_2^2 + \boldsymbol{w_3^2} + w_4^2 + w_5^2 + w_6^2 & = 0.2^2 + 0.5^2 + \boldsymbol{5^2} + 1^2 + 0.25^2 + 0.75^2 \\
 & = 0.04 + 0.25 + \boldsymbol{25} + 1 + 0.0625 + 0.5625\\ 
 & = 26.915
\end{aligned}
$$

但是， $(w_3)$ （上文加粗）的平方值为 25，几乎贡献了所有复杂度。所有其他五种权重的平方和仅为 *L2* 正则化项加上 1.915。


### Lambda

模型开发者通过以下方式来调整正则化项的整体影响：用正则化项的值乘以名为 **lambda**（也称为**正则化率**）的标量。也就是说，模型开发者旨在执行以下操作：

$$\text{minimize(Loss(Data|Model)} + \lambda \text{ complexity(Model))}$$

执行 *L2* 正则化会对模型产生以下影响

+   使权重值接近于 0（但并非正好为 0）
+   使权重的平均值接近 0，并且呈正态（钟形或高斯）分布。

增加 lambda 值会增强正则化效果。例如，lambda 值较高的权重直方图可能如图 2 所示。

![模型权重的直方图，平均值为零且呈正态分布。](https://cdn.jsdelivr.net/gh/makaspacex/PictureZone@main/uPic/image_1715655460528.png)

**图 2. 权重直方图。**

降低 lambda 的值往往会生成一个更扁平的直方图，如图 3 所示。

![模型的权重直方图，均值为零，介于平坦分布和正态分布之间。](https://cdn.jsdelivr.net/gh/makaspacex/PictureZone@main/uPic/image_1715655477510.png)


**图 3. 由较低的 lambda 值生成的权重直方图。**

选择 lambda 值时，目标是在简单性和训练数据拟合之间取得适当的平衡：

+   如果 lambda 值过高，则模型会非常简单，但是您将面临数据欠拟合的风险。模型将无法从训练数据中获得足够的信息来做出有用的预测。
    
+   如果 lambda 值过低，则模型会比较复杂，并且您将面临数据过拟合的风险。您的模型将了解过多训练数据的特殊性，无法泛化到新数据。
    

理想的 lambda 值生成的模型可以很好地泛化到以前未见过的新数据。遗憾的是，理想的 lambda 值取决于数据，因此您需要手动或自动进行一些 调整。

#### 点击加号图标即可了解 *L2* 正则化和学习速率。

学习速率与 lambda 之间存在密切关联。*L2* 正则化值越强，使特征权重接近于 0。较低的学习速率（使用早停法）通常会产生相同的效果，因为与 0 的距离没有那么大。因此，同时调整学习速率和 lambda 可能会产生令人困惑的效果。

**早停法**是指在模型完全收敛之前结束训练。在实践中，以[在线](https://developers.google.com/machine-learning/crash-course/production-ml-systems?hl=zh-cn)（连续）方式进行训练时，我们通常最终会获得一定程度的隐式早停法。也就是说，一些新趋势的数据尚不足以收敛。

如上所述，改变正则化参数的影响可能与学习速率或迭代次数变化的影响相混淆。一种有用的做法（在对固定批次的数据进行训练时）是进行足够多的迭代，这样提前停止不会产生任何影响。


## 稀疏性
### L1 正则化

稀疏矢量通常包含许多维度。创建[特征组合](https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture?hl=zh-cn)会导致包含更多维度。鉴于此类高维度特征向量，模型大小可能会变得庞大，并且需要大量的 RAM。

在高维度稀疏矢量中，最好尽可能使权重正好降至 `0`。正好为 0 的权重基本上会从模型中移除相应特征。将特征设为 0 可以节省 RAM，并可以减少模型中的噪声。

例如，假设某个住房数据集不仅涵盖加利福尼亚州，而且涵盖全球。如果按分钟（每度 60 分钟）对全球纬度进行分桶，则会在稀疏编码中获得大约 10,000 个维度；如果按分钟对全球经度进行分桶，则会产生大约 20,000 个维度。这两个特征的特征组合会产生大约 2 亿个维度。这 2 亿个维度中的许多维度都表示居住地非常有限的区域（例如海洋中部），很难利用这些数据进行有效泛化。如果代付存储这些不需要的维度的 RAM 开销就太不明智了。因此，最好是将无意义维度的权重降至正好 0，从而避免在推断时为这些模型系数的存储成本付费。

我们或许能够通过添加适当选择的正则化项，将此想法编码为训练时解决的优化问题。

L2 正则化能完成此任务吗？很遗憾，不会。 L2 正则化可以使权重变小，但并不能使其正好为 0.0。

另一种方法是尝试创建一个正则化项，减少模型中非零系数值的数量。只有在模型能够与数据拟合时，增加此计数才有意义。遗憾的是，虽然这种基于计数的方法看起来很有吸引力，但它会将我们的凸优化问题转变为非凸优化问题。因此，L0 正则化这种想法在实践中并不现实。

不过，L1 正则化这种正则化项的作用近似于 L0，但它具有凸优化的优势，因此可以有效地进行计算。因此，我们可以使用 L1 正则化使模型中很多信息缺乏的系数正好为 0，从而在推断时节省 RAM。

### L1 与 L2 正则化。

L2 和 L1 采用不同的方式降低权重：

+   L2 会降低*权重*2。
+   L1 会降低 |*权重*|。

因此，L2 和 L1 具有不同的导数：

+   L2 的导数为 2 \* *权重*。
+   L1 的导数为 k（一个常数，其值与权重无关）。

您可以将 L2 的导数的作用理解为每次移除 x% 的权重。正如 [Zeno](https://wikipedia.org/wiki/Zeno%27s_paradoxes#Dichotomy_paradox) 所说，即使将数字的 x% 按十亿次次移除，递减的数字也永远不会完全达到 0。（Zeno 不太熟悉浮点精度限制，浮点精度限制可能会导致结果正好为零。）无论如何，L2 通常不会使权重变为 0。

您可以将 L1 的导数视为每次从权重中减去一个常数的力。不过，由于绝对值，L1 在 0 处具有不连续性，这会导致与 0 相交的减法结果变为 0。例如，如果减法会使权重从 +0.1 变为 -0.2，则 L1 会将权重设为正好 0。就这样，L1 使权重变为 0。

L1 正则化（减少所有权重的绝对值）被证明对宽度模型非常有效。

请注意，该说明适用于一维模型。

点击下面的“播放”按钮 ()，比较 L1 和 L2 正则化对权重网络的影响。

<iframe src="https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/regularization-for-sparsity/reg-compare.jshtml?hl=zh-cn"
width=100% height=700px
>
</iframe>


